{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "In this lesson, we will explore multilayer perceptrons which are a basic type of neural network. We will implement them using PyTorch.\n",
    "\n",
    "**Note**: This notebook is an introduction to MLPs in PyTorch so we won't follow proper machine learning techniques to keep things short and simple (class balance in train/test splits, validation sets, early stopping, etc.).  We will implement best practices in the next notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "<img src=\"figures/mlp.png\" width=450>\n",
    "\n",
    "$z_2 = XW_1$\n",
    "\n",
    "$a_2 = f(z_2)$\n",
    "\n",
    "$z_3 = a_2W_2$\n",
    "\n",
    "$\\hat{y} = softmax(z_3)$ # classification\n",
    "\n",
    "*where*:\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W_1$ = 1st layer weights | $\\in \\mathbb{R}^{DXH}$ ($H$ is the number of hidden units in layer 1)\n",
    "* $z_2$ = outputs from first layer's weights  $\\in \\mathbb{R}^{NXH}$\n",
    "* $f$ = non-linear activation function\n",
    "* $a_2$ = activation applied first layer's outputs | $\\in \\mathbb{R}^{NXH}$\n",
    "* $W_2$ = 2nd layer weights | $\\in \\mathbb{R}^{HXC}$ ($C$ is the number of classes)\n",
    "* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NXC}$ ($N$ is the number of samples)\n",
    "\n",
    "This is a simple two-layer MLP. \n",
    "\n",
    "* **Objective:**  Predict the probability of class $y$ given the inputs $X$. Non-linearity is introduced to model the complex, non-linear data.\n",
    "* **Advantages:**\n",
    "  * Can model non-linear patterns in the data really well.\n",
    "* **Disadvantages:**\n",
    "  * Overfits easily.\n",
    "  * Computationally intensive as network increases in size.\n",
    "  * Not easily interpretable.\n",
    "* **Miscellaneous:** Future neural network architectures that we'll see use the MLP as a modular unit for feed forward operations (affine transformation (XW) followed by a non-linear operation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq65LZJbSpzd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfi_YArvjzrg"
   },
   "source": [
    "*Steps*:\n",
    "\n",
    "1. Randomly initialize the model's weights $W$ (we'll cover more effective initalization strategies in future lessons).\n",
    "2. Feed inputs $X$ into the model to do the forward pass and receive the probabilities.\n",
    "3. Compare the predictions $\\hat{y}$ (ex.  [0.3, 0.3, 0.4]]) with the actual target values $y$ (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss $J$. A common objective function for classification tasks is cross-entropy loss. \n",
    "  * $z_2 = XW_1$\n",
    "  * $a_2 = max(0, z_2)$ # ReLU activation\n",
    "  * $z_3 = a_2W_2$\n",
    "  * $\\hat{y} = softmax(z_3)$\n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) $\n",
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights. \n",
    "   * $ \\frac{\\partial{J}}{\\partial{W_{2j}}} = a_2\\hat{y}, \\frac{\\partial{J}}{\\partial{W_{2y}}} = a_2(\\hat{y}-1) $\n",
    "   * $ \\frac{\\partial{J}}{\\partial{W_1}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{a_2}}  \\frac{\\partial{a_2}}{\\partial{z_2}}  \\frac{\\partial{z_2}}{\\partial{W_1}}  = W_2(\\partial{scores})(\\partial{ReLU})X $\n",
    "   \n",
    "5. Apply backpropagation to update the weights $W$ using gradient descent. The updates will penalize the probabiltiy for the incorrect classes (j) and encourage a higher probability for the correct class (y).\n",
    "  * $W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}}$\n",
    "6. Repeat steps 2 - 4 until model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NfIz_4OPYpG"
   },
   "outputs": [],
   "source": [
    "# Now import the libraries\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKqNioAayCy"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3OrtMpFayFC"
   },
   "source": [
    "We're going to first generate some non-linear data for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdcWUP-tTHj0"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    num_samples_per_class=500,\n",
    "    dimensions=2,\n",
    "    num_classes=3,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_hidden_units=100,\n",
    "    learning_rate=1e-0,\n",
    "    regularization=1e-3,\n",
    "    num_epochs=200,\n",
    ")\n",
    "\n",
    "# Set seed for reproducability\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2O38IuNayR5"
   },
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "def generate_data(num_samples_per_class, dimensions, num_classes):\n",
    "    # Make synthetic spiral data\n",
    "    X_original = np.zeros((num_samples_per_class*num_classes, dimensions))\n",
    "    y = np.zeros(num_samples_per_class*num_classes, dtype='uint8')\n",
    "    for j in range(num_classes):\n",
    "        ix = range(num_samples_per_class*j,num_samples_per_class*(j+1))\n",
    "        r = np.linspace(0.0,1,num_samples_per_class) # radius\n",
    "        t = np.linspace(j*4,(j+1)*4,num_samples_per_class) + \\\n",
    "        np.random.randn(num_samples_per_class)*0.2 # theta\n",
    "        X_original[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "\n",
    "    # Stack\n",
    "    X = np.hstack([X_original])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q14QCqKxUS2v",
    "outputId": "78bc9bff-0db3-41ee-8297-7377c20809a8"
   },
   "outputs": [],
   "source": [
    "# Generate X & y\n",
    "X, y = generate_data(num_samples_per_class=args.num_samples_per_class, \n",
    "                     dimensions=args.dimensions, num_classes=args.num_classes)\n",
    "print (\"X: {0}\".format(np.shape(X)))\n",
    "print (\"y: {0}\".format(np.shape(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "jgVjStv8VnX2",
    "outputId": "5eb7823c-34fd-4587-b9f0-c24f7e06d2dd"
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.title(\"Generated non-linear data\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XImjkyN1MZn"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gGFqcqTDXhkl",
    "outputId": "4b9f6fd1-3d85-4015-d53f-eb89daad04a7"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "\n",
    "# Split datasets\n",
    "test_start_idx = int(len(X) * args.train_size)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHofozO7RIiV"
   },
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlVmr5XkRMCf"
   },
   "source": [
    "Before we get to our neural network, we're going to implement a linear model (logistic regression) in PyTorch first. We want to see why linear models won't suffice for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qja6kvBrRKDj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AdXiZ8ORKGS"
   },
   "outputs": [],
   "source": [
    "# Linear model\n",
    "class LogisticClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LogisticClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = self.fc1(x_in)\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ApoN49qmRhl5",
    "outputId": "0d06b9d4-3a94-4c45-81a1-d6d302734714"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LogisticClassifier(input_dim=args.dimensions, \n",
    "                           hidden_dim=args.num_hidden_units, \n",
    "                           output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ksXe8ruRhoh"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) # Adam optimizer (usually better than SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bt3kwn_3kso9"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(y_pred, y_target):\n",
    "    n_correct = torch.eq(y_pred, y_target).sum().item()\n",
    "    accuracy = n_correct / len(y_pred) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "cpo8c46ERhrS",
    "outputId": "06927f18-a3f3-4738-e94c-32b32fc3fd45"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%20==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | acc: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZthV18sPRhto"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZjKHD3zXbb0I",
    "outputId": "f6e04324-9582-4a5f-f7c9-fe5e44181126"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hsn8zbxRh09"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    \n",
    "    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
    "    y_pred = model(X_test, apply_softmax=True)\n",
    "    _, y_pred = y_pred.max(dim=1)\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "5u5fEhOcRh3V",
    "outputId": "9ebec4e7-f6f1-487c-b451-9493ffd138d9"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llXUlx5bRKJB"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC9Hqrl7RxVr"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "tlwsKnBhRxYH",
    "outputId": "26622315-45e5-44dc-8a2c-0896289eae18"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "br_O_NUCjkwm"
   },
   "source": [
    "# Non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YexKO17makzj"
   },
   "source": [
    "Now let's see how the MLP performs on the data. Note that the only difference is the addition of the non-linear activation function (we use ReLU which is just $max(0, z))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-U0_3Xco0Q4g"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) # activaton function added!\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "V-8S6w1njkDC",
    "outputId": "5ed4037a-7116-4f86-c989-906a8605bad2"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, \n",
    "            hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtZvQVVpvl5p"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "nmeU45XF0O-U",
    "outputId": "90bf9112-a477-437a-d65a-b59349498b20"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%20==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | acc: {2:.1f}%\".format(t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4tI2iZ1vl8e"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CgM4qu8I62BP",
    "outputId": "b38195c6-4b00-44b8-9ed0-3e07657ab1e7"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "Xj7cwzZ4NoVI",
    "outputId": "cdee010c-f955-4632-feda-c4038e0b9baa"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "IsxhNLTr_Jg7",
    "outputId": "5e44b5b5-c6fd-4c2d-bfdd-1914734e33c5"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3OK8p-Ng3BC"
   },
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghf5uLuhg3D0"
   },
   "source": [
    "In our MLP, we used the ReLU activation function ($max(0,z)$) which is by far the most widely use option. But there are several other options for activation functions as well, each with their own unique properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "ivnfSKEhg3Md",
    "outputId": "45b7cb4b-a87c-4bf5-dd81-4c3b1ef381ee"
   },
   "outputs": [],
   "source": [
    "# Fig size\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "# Data\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "\n",
    "# Sigmoid activation (constrain a value between 0 and 1.)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Sigmoid activation\")\n",
    "y = torch.sigmoid(x)\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Tanh activation (constrain a value between -1 and 1.)\n",
    "plt.subplot(1, 3, 2)\n",
    "y = torch.tanh(x)\n",
    "plt.title(\"Tanh activation\")\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Relu (clip the negative values to 0)\n",
    "plt.subplot(1, 3, 3)\n",
    "y = F.relu(x)\n",
    "plt.title(\"ReLU activation\")\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWv-RU46k2UL"
   },
   "source": [
    "# Initializing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hDPBE0sk2mJ"
   },
   "source": [
    "So far we have been initializing weights with small random values and this isn't optimal for convergence during training. The objective is to have weights that are able to produce outputs that follow a similar distribution across all neurons. We can do this by enforcing weights to have unit variance prior the affine and non-linear operations.\n",
    "\n",
    "A popular method is to apply [xavier initialization](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization), which essentially initializes the weights to allow the signal from the data to each deep into the network. We're going to use this in our models with PyTorch.\n",
    "\n",
    "You may be wondering why we don't do this for every forward pass and that's a great question. We'll look at more advanced strategies that help with optimization like batch/layer normalization, etc. in future lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOYptFo7k-JI"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu')) \n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) # activaton function added!\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ijvHwcZg8mO"
   },
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIhvdD_zg8os"
   },
   "source": [
    "Though neural networks are great at capturing non-linear relationships they are highly susceptible to overfitting to the training data and failing to generalize on test data. Just take a look at the example below where we generate completely random data and are able to fit a model with [$2*N*C + D$](https://arxiv.org/abs/1611.03530) hidden units. The training performance is great but the overfitting leads to very poor test performance. We'll be covering strategies to tackle overfitting in future lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRdM16NhazJP"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    num_samples_per_class=40,\n",
    "    dimensions=2,\n",
    "    num_classes=3,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_hidden_units=2*40*3+2 , # 2*N*C + D\n",
    "    learning_rate=1e-3,\n",
    "    regularization=1e-3,\n",
    "    num_epochs=1000,\n",
    ")\n",
    "\n",
    "# Set seed for reproducability\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qf00Biq6g8ty",
    "outputId": "fb7c0f5c-b15e-402e-ebce-a0260118aaea"
   },
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "X = torch.randn(args.num_samples_per_class*args.num_classes, args.dimensions).float()\n",
    "y = torch.LongTensor([[i]*args.num_samples_per_class \n",
    "                       for i in range(args.num_classes)]).view(-1)\n",
    "print (\"X: {0}\".format(np.shape(X)))\n",
    "print (\"y: {0}\".format(np.shape(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-bA9vK9SWkjh",
    "outputId": "4d304402-2504-4c3d-9388-5260c0868df0"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "\n",
    "# Split datasets\n",
    "test_start_idx = int(len(X) * args.train_size)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-_8b7AlaFdY"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        print \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) \n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xozz2bBoWkmq",
    "outputId": "be6b5de8-f06e-4407-c872-5b5fd3b6d164"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXnkWoPaWkpe"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "zgayj4E1WksH",
    "outputId": "ebba47f1-25de-4a51-f6d8-cda697c500ea"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%100==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | accuracy: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3OJLNwuZxtk"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_LU9Wzt0ZxwI",
    "outputId": "0586e063-e940-495b-b872-ace35455a4cb"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "rpSoAEdGWku5",
    "outputId": "85df95a1-c511-407b-f28e-e92cbc08a214"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "_4m9TXpTZ22C",
    "outputId": "bab12967-1565-49e0-b139-06d67b803fed"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAyjh3bieLjn"
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6kLwcBveLy5"
   },
   "source": [
    "A great technique to overcome overfitting is to increase the size of your data but this isn't always an option. Fortuntely, there are methods like regularization and dropout that can help create a more robust model. We've already seen regularization and we can easily add it in our optimizer to use it in PyTorch. \n",
    "\n",
    "Dropout is a technique (used only during training) that allows us to zero the outputs of neurons. We do this for p% of the total neurons in each layer and it changes every batch. Dropout prevents units from co-adapting too much to the data and acts as a sampling strategy since we drop a different set of neurons each time.\n",
    "\n",
    "<img src=\"figures/dropout.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGQq0MvcgBEG"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args.dropout_p = 0.1 # 40% of the neurons are dropped each pass\n",
    "args.lambda_l2 = 1e-4 # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6NvhBUyf27y"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p) # Defining the dropout\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        z = F.relu(self.fc1(x_in))\n",
    "        z = self.dropout(z) # dropping neurons\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "XQK9h7BNf26K",
    "outputId": "3cc6e22b-2c63-4799-fac9-96673f4c7fbe"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, \n",
    "            hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes, \n",
    "            dropout_p=args.dropout_p)\n",
    "print (model.named_modules)\n",
    "\n",
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, \n",
    "                       weight_decay=args.lambda_l2) # Adding L2 regularization\n",
    "\n",
    "# Training\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_Multilayer_Perceptron",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
