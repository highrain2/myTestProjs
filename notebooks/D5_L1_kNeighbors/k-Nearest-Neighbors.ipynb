{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have seen many examples for classifying data.  One of the simplest algorithms is K-Nearest Neighbors (KNN).\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a method used for **classification** or **regression**. In both cases, the input consists of the k closest training examples in the feature space.  Given a new, unknown observation, look up which points have the closest features and assign the predominant class.\n",
    "\n",
    "We begin with the standard imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "n_samples, n_features = iris.data.shape\n",
    "print(iris.keys())\n",
    "print((n_samples, n_features))\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'sepal width (cm)'\n",
    "x_index = 1\n",
    "# 'petal length (cm)'\n",
    "y_index = 2\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3))\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets, linear_model\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_iris_knn():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                        # avoid this ugly slicing by using a two-dim dataset\n",
    "    y = iris.target\n",
    "\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure()\n",
    "    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    pl.xlabel('sepal length (cm)')\n",
    "    pl.ylabel('sepal width (cm)')\n",
    "    pl.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# create the model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "\n",
    "# fit the model\n",
    "knn.fit(X, y)\n",
    "\n",
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "X_pred = [3, 5, 4, 2]\n",
    "result = knn.predict([X_pred, ])\n",
    "\n",
    "print(iris.target_names[result])\n",
    "print(iris.target_names)\n",
    "print(knn.predict_proba([X_pred, ]))\n",
    "\n",
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we see overfitting in the K-Nearest Neighbors model above.  We'll be addressing overfitting and model validation in a later notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
