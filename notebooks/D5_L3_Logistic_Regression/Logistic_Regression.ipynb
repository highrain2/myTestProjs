{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "In a previous lesson, we saw how linear regression works well for predicting continuous outputs that can easily fit to a line/plane. But linear regression doesn't fare well for classification. Alternatively, logistic regression enables us to probabilistically determine the outcome for a given set on inputs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "<img src=\"figures/logistic.jpg\" width=350>\n",
    "\n",
    "$ \\hat{y} = \\frac{1}{1 + e^{-XW}} $ \n",
    "\n",
    "*where*:\n",
    "* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DX1}$ \n",
    "\n",
    "This is the binomial logistic regression. The main idea is to take the outputs from the linear equation ($z=XW$) and use the sigmoid (logistic) function ($\\frac{1}{1+e^{-z}}$) to restrict the value between (0, 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcFvkklZSZr9"
   },
   "source": [
    "When we have more than two classes, we need to use multinomial logistic regression (softmax classifier). The softmax classifier will use the linear equation ($z=XW$) and normalize it to product the probabiltiy for class y given the inputs.\n",
    "\n",
    "$ \\hat{y} = \\frac{e^{XW_y}}{\\sum e^{XW}} $ \n",
    "\n",
    "*where*:\n",
    "* $\\hat{y}$ = prediction | $\\in \\mathbb{R}^{NX1}$ ($N$ is the number of samples)\n",
    "* $X$ = inputs | $\\in \\mathbb{R}^{NXD}$ ($D$ is the number of features)\n",
    "* $W$ = weights | $\\in \\mathbb{R}^{DXC}$ ($C$ is the number of classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4Y55tpzIjOa"
   },
   "source": [
    "* **Objective:**  Predict the probability of class $y$ given the inputs $X$. The softmax classifier normalizes the linear outputs to determine class probabilities. \n",
    "* **Advantages:**\n",
    "  * Can predict class probabilities given a set on inputs.\n",
    "* **Disadvantages:**\n",
    "  * Sensitive to **outliers** (not **noise**) since objective is minimize cross entropy loss. (Support vector machines ([SVMs](https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f)) are a good alternative to counter outliers).\n",
    "* **Miscellaneous:** Softmax classifier is going to used widely in neural network architectures as the last layer since it produces class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq65LZJbSpzd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HBPn8zPTQfZ"
   },
   "source": [
    "*Steps*:\n",
    "\n",
    "1. Randomly initialize the model's weights $W$.\n",
    "2. Feed inputs $X$ into the model to receive the logits ($z=XW$). Apply the softmax operation on the logits to get the class probabilies $\\hat{y}$ in one-hot encoded form. For example, if there are three classes, the predicted class probabilities could look like [0.3, 0.3, 0.4]. \n",
    "3. Compare the predictions $\\hat{y}$ (ex.  [0.3, 0.3, 0.4]]) with the actual target values $y$ (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss $J$. A common objective function for logistics regression is cross-entropy loss. \n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) $\n",
    "   * $y$ = [0, 0, 1]\n",
    "  * $\\hat{y}$ = [0.3, 0.3, 0.4]]\n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) = - \\sum_i [0 * ln(0.3) + 0 * ln(0.3) + 1 * ln(0.4)] = -ln(0.4) $\n",
    "  * This simplifies our cross entropy objective to the following: $J(\\theta) = - ln(\\hat{y_i})$ (negative log likelihood).\n",
    "  * $J(\\theta) = - ln(\\hat{y_i}) = - ln (\\frac{e^{X_iW_y}}{\\sum_i e^{X_iW}}) $\n",
    "4. Calculate the gradient of loss $J(\\theta)$ w.r.t to the model weights. Let's assume that our classes are mutually exclusive (a set of inputs could only belong to one class).\n",
    " * $\\frac{\\partial{J}}{\\partial{W_j}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}{(\\sum e^{XW})^2} = \\frac{Xe^{W_jX}}{\\sum e^{XW}} = XP$\n",
    "  * $\\frac{\\partial{J}}{\\partial{W_y}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}X - e^{W_yX}e^{W_yX}X}{(\\sum e^{XW})^2} = \\frac{1}{P}(XP - XP^2) = X(P-1)$\n",
    "5. Apply backpropagation to update the weights $W$ using gradient descent. The updates will penalize the probabiltiy for the incorrect classes (j) and encourage a higher probability for the correct class (y).\n",
    "  * $W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}}$\n",
    "6. Repeat steps 2 - 4 until model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H385V4VUtWOv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_hKrjzdtTgM"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyccHrQztVEu"
   },
   "source": [
    "We're going to the load the titanic dataset we looked at in a prior lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pL67TlZO6Zg4"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"data/titanic.csv\",\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "# Set seed for reproducability\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "7alqmyzXtgE8",
    "outputId": "353702e3-76f7-479d-df7a-5effcc8a7461"
   },
   "outputs": [],
   "source": [
    "# Read from CSV to Pandas DataFrame\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-5Y4zLIoE6s"
   },
   "source": [
    "# Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILkbyBHQoIwE"
   },
   "source": [
    "**Note**: The `LogisticRegression` class in Scikit-learn uses coordinate descent to solve the fit. However, we are going to use Scikit-learn's `SGDClassifier` class which uses stochastic gradient descent. We want to use this optimization approach because we will be using this for the models in subsequent lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1MJODStIu8V"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kItBIOOCTi6p"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(df):\n",
    "  \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Drop text based features (we'll learn how to use them in later lessons)\n",
    "    features_to_drop = [\"name\", \"cabin\", \"ticket\"]\n",
    "    df = df.drop(features_to_drop, axis=1)\n",
    "\n",
    "    # pclass, sex, and embarked are categorical features\n",
    "    categorical_features = [\"pclass\",\"embarked\",\"sex\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "QwQHDh4xuYTB",
    "outputId": "153ea757-b817-406d-dbde-d1fba88f194b"
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "df = preprocess(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wsGRZNNiUTqj",
    "outputId": "c9364be7-3cae-487f-9d96-3210b3129199"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "mask = np.random.rand(len(df)) < args.train_size\n",
    "train_df = df[mask]\n",
    "test_df = df[~mask]\n",
    "print (\"Train size: {0}, test size: {1}\".format(len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZKxFmATU95M"
   },
   "source": [
    "**Note**: If you have preprocessing steps like standardization, etc. that are calculated, you need to separate the training and test set first before spplying those operations. This is because we cannot apply any knowledge gained from the test set accidentally during preprocessing/training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLzL_LJd4vQ-"
   },
   "outputs": [],
   "source": [
    "# Separate X and y\n",
    "X_train = train_df.drop([\"survived\"], axis=1)\n",
    "y_train = train_df[\"survived\"]\n",
    "X_test = test_df.drop([\"survived\"], axis=1)\n",
    "y_test = test_df[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AdTYbV472UNJ",
    "outputId": "214a8114-3fd3-407f-cd6e-5f5d07294f50"
   },
   "outputs": [],
   "source": [
    "# Standardize the data (mean=0, std=1) using training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Apply scaler on training and test data (don't standardize outputs for classification)\n",
    "standardized_X_train = X_scaler.transform(X_train)\n",
    "standardized_X_test = X_scaler.transform(X_test)\n",
    "\n",
    "# Check\n",
    "print (\"mean:\", np.mean(standardized_X_train, axis=0)) # mean should be ~0\n",
    "print (\"std:\", np.std(standardized_X_train, axis=0))   # std should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-vm9AZm1_f9"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "log_reg = SGDClassifier(loss=\"log\", penalty=\"none\", max_iter=args.num_epochs, \n",
    "                        random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0e8U9NNluYVp",
    "outputId": "c5f22ade-bb8c-479b-d300-98758a82d396"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "log_reg.fit(X=standardized_X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hA7Oz97NAe8A",
    "outputId": "ab8a878a-6012-4727-8cd1-40bc5c69245b"
   },
   "outputs": [],
   "source": [
    "# Probabilities\n",
    "pred_test = log_reg.predict_proba(standardized_X_test)\n",
    "print (pred_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-jZtTd7F6_ps",
    "outputId": "d2306e4c-88a4-4ac4-9ad5-879fa461617f"
   },
   "outputs": [],
   "source": [
    "# Predictions (unstandardize them)\n",
    "pred_train = log_reg.predict(standardized_X_train) \n",
    "pred_test = log_reg.predict(standardized_X_test)\n",
    "print (pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dM7iYW8ANYjy"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFXbczqu8Rno"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sEjansj78Rqe",
    "outputId": "f5bfbe87-12c9-4aa5-fc61-e615ad4e63d4"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_acc = accuracy_score(y_train, pred_train)\n",
    "test_acc = accuracy_score(y_test, pred_test)\n",
    "print (\"train acc: {0:.2f}, test acc: {1:.2f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WijzY-vDNbE9"
   },
   "source": [
    "So far we looked at accuracy as the metric that determines our mode's level of performance. But we have several other options when it comes to  evaluation metrics.\n",
    "\n",
    "<img src=\"figures/metrics.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80MwyE0yOr-k"
   },
   "source": [
    "The metric we choose really depends on the situation.\n",
    "positive - true, 1, tumor, issue, etc., negative - false, 0, not tumor, not issue, etc.\n",
    "\n",
    "$\\text{accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$ \n",
    "\n",
    "$\\text{recall} = \\frac{TP}{TP+FN}$ → (how many of the actual issues did I actually catch)\n",
    "\n",
    "$\\text{precision} = \\frac{TP}{TP+FP}$ → (out of all the things I said were issues, how many were actually issues)\n",
    "\n",
    "$F_1 = 2 * \\frac{\\text{precision }  *  \\text{ recall}}{\\text{precision } + \\text{ recall}}$\n",
    "\n",
    "where: \n",
    "* TP: # of samples predicted to be positive and were actually positive\n",
    "* TN: # of samples predicted to be negative and were actually negative\n",
    "* FP: # of samples predicted to be positive but were actually negative\n",
    "* FN: # of samples predicted to be negative but were actually positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opmu3hJm9LXA"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAzOL8h29m82"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "KqUVzahQ-5ic",
    "outputId": "bff8819e-3d5b-45b9-c221-179c873140b1"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[\"died\", \"survived\"])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMk7tN1h98x9"
   },
   "source": [
    "When we have more than two labels (binary), we have options to calculate the evaluation metrics at a micro/macro level (per clas label), weighted, etc. You can find out more by looking at the [offical docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v6zc1_1PWnz"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zl9euDuMPYTN"
   },
   "source": [
    "Now let's see if John Doe would've survived the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "kX9428-EPUzx",
    "outputId": "ef100af7-9861-4900-e9c7-ed6d93c69069"
   },
   "outputs": [],
   "source": [
    "# Input your information\n",
    "X_infer = pd.DataFrame([{\"name\": \"John Doe\", \"cabin\": \"E\", \"ticket\": \"E44\", \n",
    "                         \"pclass\": 1, \"age\": 24, \"sibsp\": 1, \"parch\": 2, \n",
    "                         \"fare\": 100, \"embarked\": \"C\", \"sex\": \"male\"}])\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "c6OAAQoaWxAb",
    "outputId": "85eb1c6d-6f53-4bd4-bcc3-90d9ebca74c8"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "X_infer = preprocess(X_infer)\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "48sj5A0mX5Yw",
    "outputId": "d9571238-70ab-427d-f80c-7b13b00efc95"
   },
   "outputs": [],
   "source": [
    "# Add missing columns\n",
    "missing_features = set(X_test.columns) - set(X_infer.columns)\n",
    "for feature in missing_features:\n",
    "    X_infer[feature] = 0\n",
    "\n",
    "# Reorganize header\n",
    "X_infer = X_infer[X_train.columns]\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP_i8w9IXFiM"
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "standardized_X_infer = X_scaler.transform(X_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7O5PbOAvXTzF",
    "outputId": "f1c3597e-1676-476f-e970-168e5c3fca6c"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_infer = log_reg.predict_proba(standardized_X_infer)\n",
    "classes = {0: \"died\", 1: \"survived\"}\n",
    "_class = np.argmax(y_infer)\n",
    "print (\"Looks like John would've {0} with about {1:.0f}% probability on the Titanic expedition!\".format(\n",
    "    classes[_class], y_infer[0][_class]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PLPFFP67tvL"
   },
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jv6LKNXO7uch"
   },
   "source": [
    "Which of the features are most influential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KTSpxbwy7ugl",
    "outputId": "b37bf39c-f35d-4793-a479-6e61179fc5e5"
   },
   "outputs": [],
   "source": [
    "# Unstandardize coefficients \n",
    "coef = log_reg.coef_ / X_scaler.scale_\n",
    "intercept = log_reg.intercept_ - np.sum((coef * X_scaler.mean_))\n",
    "print (coef)\n",
    "print (intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJgiIupyE0Hd"
   },
   "source": [
    "A positive coefficient signifies correlation with the positive class (1=survived) and a negative coefficient signifies correlation with the negative class (0=died)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RKRB0er2C5l-",
    "outputId": "39ad0cf3-13b1-4aa8-9a6b-4456b8975a39"
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(coef)\n",
    "features = list(X_train.columns)\n",
    "print (\"Features correlated with death:\", [features[i] for i in indices[0][:3]])\n",
    "print (\"Features correlated with survival:\", [features[i] for i in indices[0][-3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhhFw3Kg-4aL"
   },
   "source": [
    "### Proof for unstandardizing coefficients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ER0HFHXj-4h8"
   },
   "source": [
    "Note that only X was standardized.\n",
    "\n",
    "$\\mathbb{E}[y] = W_0 + \\sum_{j=1}^{k}W_jz_j$\n",
    "\n",
    "$z_j = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$ \\hat{y} = \\hat{W_0} + \\sum_{j=1}^{k}\\hat{W_j}z_j $\n",
    "\n",
    "$\\hat{y} = \\hat{W_0} + \\sum_{j=1}^{k} \\hat{W}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n",
    "\n",
    "$\\hat{y} = (\\hat{W_0} - \\sum_{j=1}^{k} \\hat{W}_j\\frac{\\bar{x}_j}{\\sigma_j}) +  \\sum_{j=1}^{k} (\\frac{\\hat{w}_j}{\\sigma_j})x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yBZLVHwGKSj"
   },
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHyLTMAAGJ_x"
   },
   "source": [
    "Cross validation is a resampling techniques to evaluate a model. Instead of splitting out data once at the beginning into train/val/test sets. We do this k (usually k=5 or 10) times with different training and evaluation sets.\n",
    "\n",
    "Steps:\n",
    "1.   Shuffle the *train* dataset randomly.\n",
    "2.   Split the dataset into k discint groups.\n",
    "3.   For each iteration k, choose one of the groups to be your test set and the rest as your training set.\n",
    "4.   Repeat so that each group experiences being part of the test and train set.\n",
    "5.   Train a model using randomly initialzied weights.\n",
    "6.   After each iteration k, reinitialize the model with the same randomly initialzied weights and repeat on the new test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XB6X1b0KcvJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIqKmAEtVWMg"
   },
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "log_reg = SGDClassifier(loss=\"log\", penalty=\"none\", max_iter=args.num_epochs)\n",
    "scores = cross_val_score(log_reg, standardized_X_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_Logistic_Regression",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
