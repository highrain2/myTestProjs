{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<img src=\"figures/nn.png\" width=\"450\"/>\n",
    "\n",
    "This is the companion \"guide/implementation\" to the tutorial \"A Practitioners Guide to Neural Networks.pdf\".  \n",
    "Please go through that paper so that you understand the steps below.\n",
    "The path to learning is not simply executing code, but critical thinking, reading new material, and repetition until you understand the topic.\n",
    "However, for some students, the gap between books and implementation is rather large.\n",
    "The tutorial was written for these people, and this notebook was provided to put the code from the tutorial in one place.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "An artificial neural network (ANN) is a numerical implementation of the biological brain.\n",
    "Both \"systems\" are analogous to switches that change their output state based on the strength of the input.\n",
    "While a single switch, or neuron, produces a single output, the interconnection of thousands or millions of neurons represents a structured output.\n",
    "Through *learning*, some neurons are *triggered* differently than others, and the repetition of learning, reinforces those connections and structure.\n",
    "The reinforcement process to produce a desired result is called *feedback*.\n",
    "\n",
    "The *Artificial* neural networks attempt to simplify and mimic the biological behavior discussed above.\n",
    "*Training* an ANN takes two forms, *supervised* or *unsupervised*.\n",
    "In a *supervised* ANN, the network is trained by providing matched input and output data samples, minimizing the error between.\n",
    "For example: an e-mail spam filter might use specific text within an email to determine the authenticity of the e-mail, and the training of the ANN filter requires many examples, with iterations of feedback, before it will correctly *classify* an e-mail.\n",
    "An *unsupervised* ANN attempts to \"understand\" the structure of the input \"on its own\", requiring special *clustering* or *dimension reduction* algorithms; this method will be discussed in a future tutorial.\n",
    "\n",
    "This tutorial will walk you through the steps to build, train, and test an artificial neural network.  In our example, we will use the MNIST handwritten number dataset.\n",
    "\n",
    "Lets get started!\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy.random as r\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define some functions.\n",
    "\n",
    "The biological neuron is simulated in an ANN by an activation function (AF), or switch. If\n",
    "the input is above a user defined threshold, the AF switches state, e.g. from 0 to 1, −1 to 1\n",
    "or from 0 to > 0. A commonly used activation function is the sigmoid function.\n",
    "\n",
    "Here we define the function (which is used in the feed-forward) and its first-derivative (which we will use during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 1 / (1 + np.exp(-x)) # 1 / (1+exp(-x))\n",
    "\n",
    "def f_deriv(x):\n",
    "    return f(x) * (1 - f(x))  # f(x) * (1-f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defined the feed-forward function\n",
    "\n",
    "Feed-forward is the process of computing the output of an ANN when the weights and\n",
    "biases of the nodes are known. This is the process used after training to classify input\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    h = {1: x}\n",
    "    z = {}\n",
    "    #\n",
    "    for l in range(1, len(W) + 1):\n",
    "        # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "        # it is the output from the last layer\n",
    "        if l == 1:\n",
    "            node_in = x\n",
    "        else:\n",
    "            node_in = h[l]\n",
    "        #\n",
    "        z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "        h[l+1] = f(z[l+1]) # h^(l+1) = f(z^(l+1)) \n",
    "    #    \n",
    "    return h, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initialization functions to initialize the weights for each layer.\n",
    "\n",
    "To simplify the code, we’ll use Python dictionary objects (initialized by ). Next, initialize the weights to random values, using the NumPy function random sample, to ensure convergence during training. \n",
    "The weight initialization function is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {}\n",
    "    b = {}\n",
    "    #\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))\n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    #    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the mean accumulation values $\\Delta W$ and $\\Delta b$ to zero (these need to be the same size as the weight and bias matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    #\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    #    \n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, h_out, z_out):\n",
    "    # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-h_out) * f_deriv(z_out)\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)\n",
    "\n",
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the *training* function for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    m = len(y)\n",
    "    avg_cost_func = []\n",
    "    #\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    #\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        #    \n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        #\n",
    "        for i in range(len(y)):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored h and z values, to be used in the gradient descent step\n",
    "            h, z = feed_forward(X[i, :], W, b)\n",
    "            #\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-h[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    #\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[l][:,np.newaxis]))\n",
    "                    #\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        #\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * (1.0/m * tri_W[l])\n",
    "            b[l] += -alpha * (1.0/m * tri_b[l])\n",
    "        #\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0/m * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "        #\n",
    "    return W, b, avg_cost_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the *testing* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(W, b, X, n_layers):\n",
    "    m = X.shape[0]\n",
    "    y = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        h, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(h[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined everything, we can finally train our neural network and test it!\n",
    "\n",
    "The MNIST dataset is a standard dataset in neural network and deep learning literature.\n",
    "The dataset consists of images of hand-written digits that are labeled, or “tagged”, so that we can train and compare results of images against the true labeled value. \n",
    "Each image is of dimension 8 × 8 gray-scale pixels, a total of 64 values that indicate pixel intensity. \n",
    "We will use the Python Machine Learning library, scikit learn. \n",
    "An example of the image (and conveniently part of the scikit learn dataset) is shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "\n",
    "# plot the digit that we will test\n",
    "plt.figure()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[1]) \n",
    "#plt.savefig('nn_digit.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correctly utilize the activation function that has an x-axis sensitivity range of ±1, we need to scale our input data to a range of ±1.\n",
    "\n",
    "First consider one of the dataset pixel representations. \n",
    "Notice that the input data ranges from 0 up to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the original pixel data\n",
    "print('dataset pixel representations \\n', digits.data[0,:], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling to the range of ±1, 1$\\sigma$, using scikit learn results in the following.\n",
    "By default, scikit learn normalizes the input by subtracting the mean and dividing by the standard deviation. \n",
    "As shown, the data is centered around zero with a 1$\\sigma$ standard deviation of ±1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the scaled pixel data\n",
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "print('scale the input data \\n', X[0,:], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of ML, the term “over-fitting” implies the tendency for ANN models very accurately predict specific inputs, based on extensive training, but poorly predict inputs\n",
    "that slightly deviate from mean of the training data. Simply stated, “over-fitting” results in the inability to predict anything the ANN has not “seen” previously. \n",
    "Therefore, given a set of data, 60 − 80% of the data is used for training, while the remaining data is used for testing.\n",
    "Using scikit learn, we can split the data into training and testing sets, as show below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the digits data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict digits from 0 to 9, we need 10 nodes in the output layer. \n",
    "For example: the prediction of the digit “2” should produce the output layer result [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. \n",
    "However, in reality the result will more closely resemble [0.05, 0.05, 0.8, 0, 0, 0.05, 0, 0.02, 0.01, 0.02], which sums to 1, and the most likely value indicated by the largest value 0.8, representing the the digit “2”.\n",
    "\n",
    "The MNIST data supplied in scikit learn, the “targets” or the classification of the handwritten digits is in the form of a single number. \n",
    "We need to convert that single number into a vector so that it lines up with our 10 node output layer. \n",
    "In other words, if the target value in the dataset is “1” we want to convert it into the vector [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to vector\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "print('y_train[0]=',y_train[0])\n",
    "print('y_v_train[0]=', y_v_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will specify the structure of the neural network. \n",
    "The input layer requires 64 nodes for the 64 pixels in each image. \n",
    "The output layer requires 10 nodes to predict the digits. \n",
    "The hidden layer requires enough nodes to account for the complexity of the data. \n",
    "Using the relation $\\frac{N_{input}}{2}$, where $N_{input}$ represents the number of nodes in the input layer, define 30 nodes for the hidden layer. \n",
    "Therefore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the neural net structure\n",
    "#    64 nodes to cover the 64 pixels in the image \n",
    "#    30 hidden layer nodes to allow for the complexity of the task\n",
    "#    10 output layer nodes to predict the digits\n",
    "nn_structure = [64, 30, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we start training the ANN.\n",
    "Notice that the function above does not terminate upon reaching a threshold.\n",
    "Instead, the function terminates at $3,000$ iterations so that we can monitor the change in the average cost function, see $avg\\_cost\\_func$.\n",
    "In each gradient descent iteration, the function cycles through each training sample ($range(len(y)$) and performs the feed-forward pass followed by the back-propagation.\n",
    "The back-propagation step is an iteration through the layers starting at the output layer and working backwards – $range(len(nn\\_structure), 0, -1)$.\n",
    "The average cost is calculated at the output layer ($l == len(nn_structure)$).\n",
    "The mean accumulation values, $\\Delta W$ and $\\Delta b$, designated as $tri\\_W$ and $tri\\_b$, are updated for every layer.\n",
    "After iterating through all training samples, accumulating the $tri_W$ and $tri_b$ values, the gradient descent step is computed to change and the values for the weight and bias are updated\n",
    "\n",
    "$$W^{(l)} = W^{(l)} - \\alpha [\\frac{1}{m} \\Delta W^{(l)}]$$\n",
    "$$b^{(l)} = b^{(l)} - \\alpha [\\frac{1}{m} \\Delta b^{(l)}]$$\n",
    "\n",
    "At termination, the function returns the trained weight and bias values, as well as the tracked average cost for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function terminates, we can plot the average cost for each iteration.\n",
    "As shown in the Figure below, by $3,000$ iterations the average cost has started to \"plateau\", implying that additional iterations are not likely to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure()\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "#plt.savefig('nn_average_cost_vs_iteration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an adequately trained MNIST neural network model, we can test a (64 pixel) input from the MNIST dataset.\n",
    "This is performed by a \\emph{single} feed-forward pass through the network using our trained weight and bias values.\n",
    "As discussed previously, we assess the prediction of the output layer by taking the node with the maximum output as the predicted digit using the $numpy.argmax$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show accuracy\n",
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('accuracy score =', accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "\n",
    "Now that you have reached the end of this tutorial, you should find that the skills that you gained here will help you as you tackle more complex topics.\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
